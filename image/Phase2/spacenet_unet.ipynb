{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_PATH = \"/scratch/ssd002/datasets/cv_project/spacenet\"\n",
    "EXAMPLE_DATA_PATH = os.path.join(BASE_DATA_PATH, \"AOI_4_Shanghai_Train_processed\")\n",
    "IMG_PATH = os.path.join(BASE_DATA_PATH, \"AOI_4_Shanghai_Train_processed\", \"RGB-PanSharpen\")\n",
    "MASK_PATH = os.path.join(BASE_DATA_PATH, \"AOI_4_Shanghai_Train_processed\", \"masks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-4\n",
    "IMG_DIM = (256, 256)\n",
    "DEVICE = device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AOI_4_Shanghai_Train', 'AOI_3_Paris_Test_public', 'AOI_5_Khartoum_Test_public', 'AOI_4_Shanghai_Test_public', 'AOI_5_Khartoum_Test_public_processed', 'AOI_3_Paris_Train', 'AOI_4_Shanghai_Train_processed', 'AOI_3_Paris_Train_processed', 'AOI_2_Vegas_Train_processed', 'AOI_4_Shanghai_Test_public_processed', 'AOI_2_Vegas_Test_public_processed', 'AOI_2_Vegas_Train', 'AOI_5_Khartoum_Train_processed', 'AOI_5_Khartoum_Train', 'AOI_3_Paris_Test_public_processed', 'AOI_2_Vegas_Test_public']\n",
      "['masks', 'extras', 'RGB-PanSharpen']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(BASE_DATA_PATH))\n",
    "print(os.listdir(EXAMPLE_DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4582\n",
      "4582\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(EXAMPLE_DATA_PATH + \"/RGB-PanSharpen\")))\n",
    "print(len(os.listdir(EXAMPLE_DATA_PATH + \"/masks\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RGB-PanSharpen_AOI_4_Shanghai_img1001.png', 'RGB-PanSharpen_AOI_4_Shanghai_img1002.png', 'RGB-PanSharpen_AOI_4_Shanghai_img1003.png', 'RGB-PanSharpen_AOI_4_Shanghai_img1005.png', 'RGB-PanSharpen_AOI_4_Shanghai_img1007.png', 'RGB-PanSharpen_AOI_4_Shanghai_img1008.png', 'RGB-PanSharpen_AOI_4_Shanghai_img1009.png', 'RGB-PanSharpen_AOI_4_Shanghai_img1010.png', 'RGB-PanSharpen_AOI_4_Shanghai_img1012.png', 'RGB-PanSharpen_AOI_4_Shanghai_img1013.png', 'RGB-PanSharpen_AOI_4_Shanghai_img1014.png', 'RGB-PanSharpen_AOI_4_Shanghai_img1015.png', 'RGB-PanSharpen_AOI_4_Shanghai_img1016.png', 'RGB-PanSharpen_AOI_4_Shanghai_img1017.png', 'RGB-PanSharpen_AOI_4_Shanghai_img1018.png']\n",
      "['AOI_4_Shanghai_img1001_mask.png', 'AOI_4_Shanghai_img1002_mask.png', 'AOI_4_Shanghai_img1003_mask.png', 'AOI_4_Shanghai_img1005_mask.png', 'AOI_4_Shanghai_img1007_mask.png', 'AOI_4_Shanghai_img1008_mask.png', 'AOI_4_Shanghai_img1009_mask.png', 'AOI_4_Shanghai_img1010_mask.png', 'AOI_4_Shanghai_img1012_mask.png', 'AOI_4_Shanghai_img1013_mask.png', 'AOI_4_Shanghai_img1014_mask.png', 'AOI_4_Shanghai_img1015_mask.png', 'AOI_4_Shanghai_img1016_mask.png', 'AOI_4_Shanghai_img1017_mask.png', 'AOI_4_Shanghai_img1018_mask.png']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(os.listdir(EXAMPLE_DATA_PATH + \"/RGB-PanSharpen\"))[:15])\n",
    "print(sorted(os.listdir(EXAMPLE_DATA_PATH + \"/masks\"))[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650, 650, 3)\n"
     ]
    }
   ],
   "source": [
    "ex_img_path = os.path.join(EXAMPLE_DATA_PATH, \"RGB-PanSharpen\", \"RGB-PanSharpen_AOI_4_Shanghai_img2070.png\")\n",
    "ex_img = np.array(Image.open(ex_img_path))\n",
    "print(ex_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650, 650)\n"
     ]
    }
   ],
   "source": [
    "ex_mask_path = os.path.join(EXAMPLE_DATA_PATH, \"masks\", \"AOI_4_Shanghai_img2070_mask.png\")\n",
    "ex_mask = np.array(Image.open(ex_mask_path))\n",
    "print(ex_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f53b35d1898>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, axarr = plt.subplots(1, 2, figsize=(20, 20))\n",
    "axarr[0].imshow(ex_img)\n",
    "axarr[1].imshow(ex_mask, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpaceNet_Dataset(Dataset): \n",
    "    def __init__(self, img_dir, mask_dir, transform = None):\n",
    "        self.img_dir = img_dir \n",
    "        self.mask_dir = mask_dir\n",
    "        self.images = os.listdir(img_dir)\n",
    "        self.masks = os.listdir(mask_dir)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.img_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[index])\n",
    "        \n",
    "        img = Image.open(img_path)\n",
    "        mask = Image.open(mask_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        return img, mask\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.maxpool_2x2 = nn.MaxPool2d(kernel_size= 2, stride= 2)\n",
    "        self.dconv1= double_conv(in_channels,64)\n",
    "        self.dconv2= double_conv(64,128)\n",
    "        self.dconv3= double_conv(128,256)\n",
    "        self.dconv4= double_conv(256,512)\n",
    "        self.dconv5= double_conv(512,1024)\n",
    "        self.num_of_classes = out_channels\n",
    "        \n",
    "        #Now, the first up convolution is performed followed by a double convolution to alter the number of channels of feature map.\n",
    "        self.uptrans1= nn.ConvTranspose2d(\n",
    "            in_channels= 1024,\n",
    "            out_channels= 512,\n",
    "            kernel_size= 2,\n",
    "            stride= 2\n",
    "        )\n",
    "        \n",
    "        self.upconv1= double_conv(1024,512)\n",
    "        \n",
    "        self.uptrans2= nn.ConvTranspose2d(\n",
    "            in_channels= 512,\n",
    "            out_channels= 256,\n",
    "            kernel_size= 2,\n",
    "            stride= 2\n",
    "        )\n",
    "        \n",
    "        self.upconv2= double_conv(512, 256)\n",
    "        \n",
    "        self.uptrans3= nn.ConvTranspose2d(\n",
    "            in_channels= 256,\n",
    "            out_channels= 128,\n",
    "            kernel_size= 2,\n",
    "            stride= 2\n",
    "        )\n",
    "        \n",
    "        self.upconv3= double_conv(256,128)\n",
    "        \n",
    "        self.uptrans4= nn.ConvTranspose2d(\n",
    "            in_channels= 128,\n",
    "            out_channels= 64,\n",
    "            kernel_size= 2,\n",
    "            stride= 2\n",
    "        )\n",
    "        \n",
    "        self.upconv4= double_conv(128,64)\n",
    "        \n",
    "        self.out= nn.Conv2d(\n",
    "            in_channels= 64,\n",
    "            out_channels= self.num_of_classes,\n",
    "            kernel_size= 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, image):\n",
    "        \n",
    "        #encoder\n",
    "        enc_x_1= self.dconv1(image)\n",
    "        enc_x_2= self.maxpool_2x2(enc_x_1)\n",
    "        enc_x_3= self.dconv2(enc_x_2)\n",
    "        enc_x_4= self.maxpool_2x2(enc_x_3)\n",
    "        enc_x_5= self.dconv3(enc_x_4)\n",
    "        enc_x_6= self.maxpool_2x2(enc_x_5)\n",
    "        enc_x_7= self.dconv4(enc_x_6)\n",
    "        enc_x_8= self.maxpool_2x2(enc_x_7)\n",
    "        enc_x_9= self.dconv5(enc_x_8)\n",
    "        \n",
    "        #decoder\n",
    "        dec_x_1= self.uptrans1(enc_x_9)\n",
    "        dec_x_2 = self.upconv1(torch.cat([dec_x_1, enc_x_7],1))\n",
    "        \n",
    "        dec_x_3= self.uptrans2(dec_x_2)\n",
    "        dec_x_4= self.upconv2(torch.cat([dec_x_3, enc_x_5],1))\n",
    "        \n",
    "        dec_x_5= self.uptrans3(dec_x_4)\n",
    "        dec_x_6= self.upconv3(torch.cat([dec_x_5, enc_x_3],1))\n",
    "        \n",
    "        dec_x_7= self.uptrans4(dec_x_6)\n",
    "        dec_x_8= self.upconv4(torch.cat([dec_x_7, enc_x_1],1))\n",
    "        \n",
    "        dec_x_9= self.out(dec_x_8)\n",
    "        print(dec_x_9.size())\n",
    "        return dec_x_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(in_c, out_c):\n",
    "    \n",
    "    conv= nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size= 3, padding=(1, 1)),\n",
    "        nn.ReLU(inplace= False),\n",
    "        nn.Conv2d(out_c, out_c, kernel_size= 3, padding=(1, 1)),\n",
    "        nn.ReLU(inplace= False)\n",
    "    )\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(loader)\n",
    "    \n",
    "    for batch_id, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.float().to(device=DEVICE)\n",
    "            \n",
    "        predictions = model(data)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    normalize = transforms.Normalize(mean=[0], std=[1])\n",
    "        \n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_DIM),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "    ])\n",
    "    \n",
    "    train_ds = SpaceNet_Dataset(img_dir=IMG_PATH, \n",
    "                                mask_dir=MASK_PATH, \n",
    "                                transform=data_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=2,\n",
    "                              pin_memory=True,\n",
    "                              shuffle=True\n",
    "                              )\n",
    "    \n",
    "    model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_fn(train_loader, model, optimizer, loss_fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/144 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/144 [00:03<07:34,  3.18s/it, loss=0.668]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/144 [00:04<06:18,  2.67s/it, loss=0.661]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/144 [00:06<05:22,  2.29s/it, loss=0.66] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/144 [00:07<04:43,  2.02s/it, loss=0.659]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 5/144 [00:08<04:16,  1.84s/it, loss=0.651]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 6/144 [00:10<03:57,  1.72s/it, loss=0.644]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 7/144 [00:11<03:43,  1.63s/it, loss=0.64] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 8/144 [00:13<03:32,  1.57s/it, loss=0.629]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 9/144 [00:14<03:25,  1.52s/it, loss=0.636]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 10/144 [00:16<03:20,  1.49s/it, loss=0.612]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-0ed759848192>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-6bbdb7b91cf2>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(loader, model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
